{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoTheta, AutoCES, AutoTBATS\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THE SYNTHETIC DATA GENERATION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_only_df['unique_id'] = 'price_only'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of periods to forecast ahead\n",
    "forecast_horizon = 3\n",
    "\n",
    "# Size of each rolling window step\n",
    "step_size = 3\n",
    "\n",
    "# Total number of rolling windows for cross-validation\n",
    "n_windows = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 23:46:46,096\tINFO worker.py:1781 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray for parallel processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Define the models and forecaster\n",
    "season_length = 12  # annual seasonality for monthly data\n",
    "# List of statistical forecasting models with seasonal components\n",
    "models = [\n",
    "    AutoARIMA(season_length=season_length),  # Automated ARIMA model selection\n",
    "    AutoETS(season_length=season_length),    # Automated Exponential Smoothing\n",
    "    AutoTheta(season_length=season_length),  # Automated Theta method\n",
    "    AutoCES(season_length=season_length)     # Automated Complex Exponential Smoothing\n",
    "]\n",
    "\n",
    "# Create StatsForecast object with parallel processing\n",
    "def get_stats_forecaster():\n",
    "    \"\"\"\n",
    "    Creates and returns a StatsForecast object with the defined models.\n",
    "    \n",
    "    Returns:\n",
    "        StatsForecast: Configured forecaster with parallel processing enabled\n",
    "    \"\"\"\n",
    "    return StatsForecast(models=models, freq='M', n_jobs=-1)\n",
    "\n",
    "def prepare_data(df, use_scaler=False):\n",
    "    \"\"\"\n",
    "    Prepares data for forecasting by handling data types and optional scaling.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with time series data\n",
    "        use_scaler (bool): Whether to apply MinMax scaling to the target variable\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Prepared DataFrame, Fitted scaler or None if scaling not used)\n",
    "    \"\"\"\n",
    "    # Ensure 'y' column is numeric\n",
    "    df = df.copy()\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    \n",
    "    # Apply MinMax scaling if requested\n",
    "    scaler = None\n",
    "    if use_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        df['y'] = scaler.fit_transform(df[['y']])\n",
    "    \n",
    "    # Split the data into train and test sets based on rolling window parameters\n",
    "    train_size = len(df) - n_windows * step_size\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "    \n",
    "    return pd.concat([train_df, test_df]), scaler\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates multiple performance metrics for forecast evaluation.\n",
    "    \n",
    "    Args:\n",
    "        actual (array-like): True values\n",
    "        predicted (array-like): Predicted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RMSE, Directional Accuracy, Turning Point Accuracy)\n",
    "    \"\"\"\n",
    "    actual = np.asarray(actual).flatten()\n",
    "    predicted = np.asarray(predicted).flatten()\n",
    "\n",
    "    # Root Mean Square Error\n",
    "    rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # Directional Accuracy - measures correct prediction of up/down movements\n",
    "    actual_diff = np.diff(actual)\n",
    "    pred_diff = np.diff(predicted)\n",
    "    directional_accuracy = np.mean((actual_diff * pred_diff) > 0)\n",
    "    \n",
    "    # Turning Point Accuracy - measures correct prediction of trend changes\n",
    "    actual_turns = (actual_diff[:-1] * actual_diff[1:]) < 0\n",
    "    pred_turns = (pred_diff[:-1] * pred_diff[1:]) < 0\n",
    "    turning_point_accuracy = np.mean(actual_turns == pred_turns)\n",
    "    \n",
    "    return rmse, directional_accuracy, turning_point_accuracy\n",
    "\n",
    "@ray.remote\n",
    "def run_experiment(df, model_names, use_scaler=False):\n",
    "    \"\"\"\n",
    "    Runs forecasting experiment with cross-validation for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with time series data\n",
    "        model_names (list): List of model names to evaluate\n",
    "        use_scaler (bool): Whether to apply MinMax scaling\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Results DataFrame with metrics, Cross-validation DataFrame with predictions)\n",
    "    \"\"\"\n",
    "    # Initialize forecaster and prepare data\n",
    "    stats_forecaster = get_stats_forecaster()\n",
    "    prepared_df, scaler = prepare_data(df, use_scaler)\n",
    "    \n",
    "    # Prepare for cross-validation\n",
    "    cv_df = prepared_df[['ds', 'y', 'unique_id']].copy()\n",
    "    cv_df['y'] = cv_df['y'].astype(float)\n",
    "\n",
    "    # Perform rolling window cross-validation\n",
    "    crossvalidation_df = stats_forecaster.cross_validation(\n",
    "        df=cv_df,\n",
    "        h=forecast_horizon,\n",
    "        step_size=step_size,\n",
    "        n_windows=n_windows\n",
    "    )\n",
    "\n",
    "    # Inverse transform predictions if scaling was applied\n",
    "    if scaler:\n",
    "        crossvalidation_df['y'] = scaler.inverse_transform(crossvalidation_df[['y']])\n",
    "        for model in model_names:\n",
    "            if model in crossvalidation_df.columns:\n",
    "                crossvalidation_df[model] = scaler.inverse_transform(crossvalidation_df[[model]])\n",
    "\n",
    "    # Calculate performance metrics for each model\n",
    "    results = []\n",
    "    for model in model_names:\n",
    "        if model in crossvalidation_df.columns:\n",
    "            rmse, dir_acc, turn_acc = calculate_metrics(\n",
    "                crossvalidation_df['y'].values,\n",
    "                crossvalidation_df[model].values\n",
    "            )\n",
    "            \n",
    "            # Calculate weighted score (equal weights for all metrics)\n",
    "            weighted_score = (rmse + (1 - dir_acc) + (1 - turn_acc)) / 3\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model,\n",
    "                'RMSE': rmse,\n",
    "                'Directional_Accuracy': dir_acc,\n",
    "                'Turning_Point_Accuracy': turn_acc,\n",
    "                'Weighted_Score': weighted_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), crossvalidation_df\n",
    "\n",
    "# Define statistical models to evaluate\n",
    "model_names = ['AutoARIMA', 'AutoETS', 'AutoTheta', 'CES']\n",
    "\n",
    "# Run parallel experiments with and without data scaling\n",
    "experiment_ref_no_scale = run_experiment.remote(price_only_df, model_names, use_scaler=False)\n",
    "experiment_ref_with_scale = run_experiment.remote(price_only_df, model_names, use_scaler=True)\n",
    "\n",
    "# Collect results from parallel processes\n",
    "results_df_no_scale, crossvalidation_df_no_scale = ray.get(experiment_ref_no_scale)\n",
    "results_df_with_scale, crossvalidation_df_with_scale = ray.get(experiment_ref_with_scale)\n",
    "\n",
    "# Display performance metrics for both scaling approaches\n",
    "print(\"\\nModel Performance Metrics (No Scaling):\")\n",
    "print(results_df_no_scale.to_string(index=False))\n",
    "print(\"\\nModel Performance Metrics (With MinMax Scaling):\")\n",
    "print(results_df_with_scale.to_string(index=False))\n",
    "\n",
    "# Identify best performing models based on weighted score\n",
    "best_model_no_scale = results_df_no_scale.loc[results_df_no_scale['Weighted_Score'].idxmin(), 'Model']\n",
    "best_model_with_scale = results_df_with_scale.loc[results_df_with_scale['Weighted_Score'].idxmin(), 'Model']\n",
    "print(f\"\\nBest Model (No Scaling): {best_model_no_scale}\")\n",
    "print(f\"Best Model (With MinMax Scaling): {best_model_with_scale}\")\n",
    "\n",
    "# Clean up Ray resources\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
