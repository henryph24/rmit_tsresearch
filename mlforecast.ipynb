{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_only_df['unique_id'] = 'price_only'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "import multiprocessing\n",
    "from math import sqrt\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# MLForecast\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.auto import (\n",
    "    AutoMLForecast,\n",
    "    AutoElasticNet, \n",
    "    AutoXGBoost,\n",
    "    AutoLightGBM,\n",
    "    AutoCatboost\n",
    ")\n",
    "from mlforecast.target_transforms import LocalStandardScaler\n",
    "from mlforecast.lag_transforms import ExponentiallyWeightedMean, RollingMean\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from utilsforecast.plotting import plot_series\n",
    "\n",
    "# Core forecasting utilities\n",
    "from coreforecast.scalers import LocalStandardScaler, LocalMinMaxScaler\n",
    "from coreforecast.grouped_array import GroupedArray\n",
    "\n",
    "# Set up multiprocessing and seeds\n",
    "print(multiprocessing.cpu_count())\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CatBoost, we only tune subsample parameter due to limited data size\n",
    "# More parameters could cause overfitting or convergence issues\n",
    "# subsample: controls the fraction of samples used for each tree building\n",
    "def catboost_model_params(trial: optuna.Trial):\n",
    "    return {\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate multiple performance metrics for forecasting evaluation.\n",
    "    \n",
    "    Args:\n",
    "        actual (array-like): The actual/true values\n",
    "        predicted (array-like): The predicted/forecasted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - rmse (float): Root Mean Square Error\n",
    "            - directional_accuracy (float): Proportion of correctly predicted directions (0-1)\n",
    "            - turning_point_accuracy (float): Proportion of correctly predicted turning points (0-1)\n",
    "            - weighted_score (float): Combined score weighing all three metrics equally\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays and flatten\n",
    "    actual = np.asarray(actual).flatten()\n",
    "    predicted = np.asarray(predicted).flatten()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # Calculate directional accuracy (proportion of correctly predicted up/down movements)\n",
    "    actual_diff = np.diff(actual)\n",
    "    pred_diff = np.diff(predicted)\n",
    "    directional_accuracy = np.mean((actual_diff * pred_diff) > 0)\n",
    "    \n",
    "    # Calculate turning point accuracy (proportion of correctly predicted trend changes)\n",
    "    actual_turns = (actual_diff[:-1] * actual_diff[1:]) < 0  # True when direction changes\n",
    "    pred_turns = (pred_diff[:-1] * pred_diff[1:]) < 0\n",
    "    turning_point_accuracy = np.mean(actual_turns == pred_turns)\n",
    "    \n",
    "    # Calculate weighted score - lower is better\n",
    "    # Combines RMSE with penalties for poor directional and turning point accuracy\n",
    "    weighted_score = (rmse + (1 - directional_accuracy) + (1 - turning_point_accuracy)) / 3\n",
    "    \n",
    "    return rmse, directional_accuracy, turning_point_accuracy, weighted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forecasting_pipeline(with_macro_df, horizon=3, step_size=3, n_windows=16):\n",
    "    \"\"\"Run an automated machine learning forecasting pipeline with multiple models.\n",
    "    \n",
    "    This function implements a complete forecasting workflow including:\n",
    "    - Train/test splitting\n",
    "    - Data preprocessing and scaling\n",
    "    - Model training with cross-validation\n",
    "    - Prediction generation\n",
    "    - Performance evaluation and visualization\n",
    "    \n",
    "    Args:\n",
    "        with_macro_df (pd.DataFrame): Input dataframe containing target variable 'y',\n",
    "            datetime column 'ds', ID column 'unique_id' and optional macro features\n",
    "        horizon (int, optional): Number of future periods to forecast. Defaults to 3.\n",
    "        step_size (int, optional): Number of periods between cross-validation windows. Defaults to 3.\n",
    "        n_windows (int, optional): Number of cross-validation windows. Defaults to 16.\n",
    "            \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - auto_mlf (AutoMLForecast): The fitted forecasting model\n",
    "            - predictions (pd.DataFrame): Future predictions\n",
    "            - cv_results (dict): Cross-validation results for each model\n",
    "            - metrics_df (pd.DataFrame): Performance metrics comparison\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    # Test set size is determined by number of windows * step size\n",
    "    train_size = len(with_macro_df) - n_windows * step_size\n",
    "    train_df = with_macro_df[:train_size].copy()\n",
    "    test_df = with_macro_df[train_size:].copy()  \n",
    "\n",
    "    # Basic preprocessing - fill missing values with 0\n",
    "    processed_df = with_macro_df.copy()\n",
    "    processed_df.fillna(0)\n",
    "\n",
    "    # Identify any exogenous (macro) features by excluding standard columns\n",
    "    macro_features = processed_df.columns.difference(['unique_id', 'ds', 'y'])\n",
    "    has_exog = len(macro_features) > 0\n",
    "\n",
    "    # Scale macro features if present using local min-max scaling\n",
    "    if has_exog:\n",
    "        scaler = LocalMinMaxScaler()\n",
    "        \n",
    "        # First scale training data\n",
    "        for feature in macro_features:\n",
    "            train_values = train_df[feature].values\n",
    "            indptr = np.array([0, len(train_values)])\n",
    "            grouped_train = GroupedArray(train_values, indptr)\n",
    "            scaled_train_values = scaler.fit_transform(grouped_train)\n",
    "            train_df[feature] = scaled_train_values\n",
    "\n",
    "        # Then scale full dataset using fitted scaler\n",
    "        for feature in macro_features:\n",
    "            full_values = processed_df[feature].values\n",
    "            indptr = np.array([0, len(full_values)])\n",
    "            grouped_full = GroupedArray(full_values, indptr)\n",
    "            scaled_full_values = scaler.transform(grouped_full)\n",
    "            processed_df[feature] = scaled_full_values\n",
    "\n",
    "    # Initialize dictionary of models to evaluate\n",
    "    models = {\n",
    "        'elasticnet': AutoElasticNet(),  # Linear model with L1/L2 regularization\n",
    "        'xgboost': AutoXGBoost(),        # Gradient boosting with trees\n",
    "        'lightgbm': AutoLightGBM(),      # Light gradient boosting\n",
    "        'catboost': AutoCatboost(config = catboost_model_params)  # Categorical boosting\n",
    "    }\n",
    "\n",
    "    # Configure automated ML forecasting framework\n",
    "    auto_mlf = AutoMLForecast(\n",
    "        models=models,\n",
    "        freq='M',  # Monthly frequency\n",
    "        season_length=12,  # Annual seasonality\n",
    "        fit_config=lambda trial: {\n",
    "            'static_features': [],\n",
    "            'dropna': True,\n",
    "            'keep_last_n': None\n",
    "        },\n",
    "        num_threads=12  # Parallel processing\n",
    "    )\n",
    "\n",
    "    # Fit models with cross-validation\n",
    "    print(\"Performing optimization and cross-validation...\")\n",
    "    auto_mlf.fit(\n",
    "        train_df,\n",
    "        n_windows=16,\n",
    "        h=3,\n",
    "        num_samples=100,\n",
    "        step_size=3\n",
    "    )\n",
    "\n",
    "    # Generate future prediction dataframe\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    any_model = next(iter(auto_mlf.models_.values()))\n",
    "    future_df = any_model.make_future_dataframe(h=horizon)\n",
    "    \n",
    "    # Handle future macro features if present\n",
    "    if has_exog:\n",
    "        # Get last known values for each series\n",
    "        last_dates = with_macro_df.groupby('unique_id')['ds'].max()\n",
    "        future_values = []\n",
    "        \n",
    "        # Create future macro data using last known values\n",
    "        for idx, row in future_df.iterrows():\n",
    "            uid = row['unique_id']\n",
    "            last_known_values = with_macro_df[with_macro_df['unique_id'] == uid].loc[\n",
    "                with_macro_df['ds'] == last_dates[uid], \n",
    "                macro_features\n",
    "            ].iloc[0]\n",
    "            \n",
    "            future_values.append({\n",
    "                'unique_id': uid,\n",
    "                'ds': row['ds'],\n",
    "                **last_known_values\n",
    "            })\n",
    "        \n",
    "        # Scale future macro features\n",
    "        future_macro_df = pd.DataFrame(future_values)\n",
    "        for feature in macro_features:\n",
    "            future_values = future_macro_df[feature].values\n",
    "            indptr = np.array([0, len(future_values)])\n",
    "            grouped_future = GroupedArray(future_values, indptr)\n",
    "            scaled_future_values = scaler.transform(grouped_future)\n",
    "            future_macro_df[feature] = scaled_future_values\n",
    "        \n",
    "        # Generate predictions with exogenous features\n",
    "        predictions = auto_mlf.predict(horizon, X_df=future_macro_df)\n",
    "    else:\n",
    "        # Generate predictions without exogenous features\n",
    "        predictions = auto_mlf.predict(horizon)\n",
    "\n",
    "    # Evaluate models using cross-validation\n",
    "    cv_results = {}\n",
    "    metrics = {}\n",
    "\n",
    "    # Loop through each model for evaluation\n",
    "    for model_name, model in auto_mlf.models_.items():\n",
    "        # Perform cross-validation on last 48 periods\n",
    "        cv_df = model.cross_validation(\n",
    "            df=processed_df,\n",
    "            n_windows=n_windows,\n",
    "            h=horizon,\n",
    "            step_size=step_size,\n",
    "            static_features=[],\n",
    "            dropna=True,\n",
    "            keep_last_n=48,\n",
    "        )\n",
    "        cv_results[model_name] = cv_df\n",
    "        actual = cv_df['y']\n",
    "        predicted = cv_df[model_name]\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        rmse, dir_acc, turn_acc, weighted_score = calculate_metrics(actual, predicted)\n",
    "        metrics[model_name] = {\n",
    "            'RMSE': rmse,\n",
    "            'Directional Accuracy': dir_acc,\n",
    "            'Turning Point Accuracy': turn_acc,\n",
    "            'Weighted Score': weighted_score\n",
    "        }\n",
    "\n",
    "        # Create evaluation plots\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        print(f\"\\nMetrics for {model_name}:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"Directional Accuracy: {dir_acc:.4f}\")\n",
    "        print(f\"Turning Point Accuracy: {turn_acc:.4f}\")\n",
    "        print(f\"Weighted Score: {weighted_score:.4f}\")\n",
    "        \n",
    "        # Print value ranges for validation\n",
    "        print(f\"\\nValue ranges for {model_name}:\")\n",
    "        print(\"Original data range:\", with_macro_df['y'].min(), \"-\", with_macro_df['y'].max())\n",
    "        print(\"Predicted data range:\", cv_df[model_name].min(), \"-\", cv_df[model_name].max())\n",
    "        print(\"Time range:\", cv_df['ds'].min(), \"-\", cv_df['ds'].max())\n",
    "\n",
    "        # Plot actual vs predicted values\n",
    "        plt.plot(cv_df['ds'], cv_df['y'], 'b.', label='Actual', alpha=0.5)\n",
    "        for uid in cv_df['unique_id'].unique():\n",
    "            mask = cv_df['unique_id'] == uid\n",
    "            plt.plot(cv_df.loc[mask, 'ds'], \n",
    "                    cv_df.loc[mask, model_name], \n",
    "                    'r-', alpha=0.3)\n",
    "        \n",
    "        plt.title(f'Cross-validation results for {model_name} (Last 48 Periods Only)\\nWeighted Score: {weighted_score:.4f}')\n",
    "        plt.ylabel('Value')\n",
    "        plt.xlabel('Time')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Set y-axis limits with padding\n",
    "        y_min = min(with_macro_df['y'].min(), cv_df[model_name].min())\n",
    "        y_max = max(with_macro_df['y'].max(), cv_df[model_name].max())\n",
    "        padding = (y_max - y_min) * 0.1\n",
    "        plt.ylim(y_min - padding, y_max + padding)\n",
    "        \n",
    "        plt.legend(['Actual', 'Predicted'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create comparison metrics dataframe\n",
    "    metrics_df = pd.DataFrame(metrics).round(4)\n",
    "    print(\"\\nModel Comparison Metrics:\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    # Identify best performing model based on weighted score\n",
    "    best_model = min(metrics.items(), key=lambda x: x[1]['Weighted Score'])\n",
    "    print(f\"\\nBest Model: {best_model[0]} (Weighted Score: {best_model[1]['Weighted Score']:.4f})\")\n",
    "\n",
    "    return auto_mlf, predictions, cv_results, metrics_df\n",
    "\n",
    "# Run the forecasting pipeline\n",
    "auto_mlf, predictions, cv_results, metrics_df = run_forecasting_pipeline(with_macro_df, horizon=3, step_size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla_package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
